The 1000k-example 37k-vocab tokenizer does 693.2563 tokens/example on average across 50K fineweb examples, which amounts to 0.999835 less (so more efficient) than the gpt2 tokenizer.

The 1000k 10k tokenizer does 795.19234 tokens/example which amounts to 1.14685 more than the gpt2 tokenizer.

The 50k tokenizer does 680.10824, so 0.98087 more efficient.

Because the gpt2 tokenizer does 693.37074 tokens/example.


